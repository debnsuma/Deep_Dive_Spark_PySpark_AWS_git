{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .appName(\"Demo App\")\n",
    "            .config(\"spark.ui.port\", \"4050\")\n",
    "            .getOrCreate()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3453b928f9b4:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Demo App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Demo App>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3453b928f9b4:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Demo App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff7527a450>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameReader at 0xffff75278e10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['csv',\n",
       " 'format',\n",
       " 'jdbc',\n",
       " 'json',\n",
       " 'load',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orc',\n",
       " 'parquet',\n",
       " 'schema',\n",
       " 'table',\n",
       " 'text']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(spark.read) if not i.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = spark.read.text(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "|most other parts ...|\n",
      "|whatsoever. You m...|\n",
      "|of the Project Gu...|\n",
      "|www.gutenberg.org...|\n",
      "|will have to chec...|\n",
      "|   using this eBook.|\n",
      "|                    |\n",
      "|Title: Pride and ...|\n",
      "|                    |\n",
      "| Author: Jane Austen|\n",
      "|                    |\n",
      "|Release Date: Nov...|\n",
      "|                    |\n",
      "|   Language: English|\n",
      "|                    |\n",
      "|Produced by: Chuc...|\n",
      "|             http...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "book.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+\n",
      "|value                                                                   |\n",
      "+------------------------------------------------------------------------+\n",
      "|The Project Gutenberg eBook of Pride and prejudice, by Jane Austen      |\n",
      "|                                                                        |\n",
      "|This eBook is for the use of anyone anywhere in the United States and   |\n",
      "|most other parts of the world at no cost and with almost no restrictions|\n",
      "|whatsoever. You may copy it, give it away or re-use it under the terms  |\n",
      "|of the Project Gutenberg License included with this eBook or online at  |\n",
      "|www.gutenberg.org. If you are not located in the United States, you     |\n",
      "|will have to check the laws of the country where you are located before |\n",
      "|using this eBook.                                                       |\n",
      "|                                                                        |\n",
      "+------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.show(10, truncate=False, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------\n",
      " value | The Project Gutenberg eBook of Pride and prejudice, by Jane Austen       \n",
      "-RECORD 1-------------------------------------------------------------------------\n",
      " value |                                                                          \n",
      "-RECORD 2-------------------------------------------------------------------------\n",
      " value | This eBook is for the use of anyone anywhere in the United States and    \n",
      "-RECORD 3-------------------------------------------------------------------------\n",
      " value | most other parts of the world at no cost and with almost no restrictions \n",
      "-RECORD 4-------------------------------------------------------------------------\n",
      " value | whatsoever. You may copy it, give it away or re-use it under the terms   \n",
      "-RECORD 5-------------------------------------------------------------------------\n",
      " value | of the Project Gutenberg License included with this eBook or online at   \n",
      "-RECORD 6-------------------------------------------------------------------------\n",
      " value | www.gutenberg.org. If you are not located in the United States, you      \n",
      "-RECORD 7-------------------------------------------------------------------------\n",
      " value | will have to check the laws of the country where you are located before  \n",
      "-RECORD 8-------------------------------------------------------------------------\n",
      " value | using this eBook.                                                        \n",
      "-RECORD 9-------------------------------------------------------------------------\n",
      " value |                                                                          \n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.show(10, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F \n",
    "\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways to select columns "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "book.select(book.value)\n",
    "book.select(book[\"value\"])\n",
    "book.select(F.col(\"value\"))\n",
    "book.select(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.select(book.value).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.select(book['value']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.select(F.col('value')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.select('value').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[most, other, par...|\n",
      "|[whatsoever., You...|\n",
      "|[of, the, Project...|\n",
      "|[www.gutenberg.or...|\n",
      "|[will, have, to, ...|\n",
      "|[using, this, eBo...|\n",
      "|                  []|\n",
      "|[Title:, Pride, a...|\n",
      "|                  []|\n",
      "|[Author:, Jane, A...|\n",
      "|                  []|\n",
      "|[Release, Date:, ...|\n",
      "|                  []|\n",
      "|[Language:, English]|\n",
      "|                  []|\n",
      "|[Produced, by:, C...|\n",
      "|[, , , , , , , , ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+\n",
      "|value                                                                   |\n",
      "+------------------------------------------------------------------------+\n",
      "|The Project Gutenberg eBook of Pride and prejudice, by Jane Austen      |\n",
      "|                                                                        |\n",
      "|This eBook is for the use of anyone anywhere in the United States and   |\n",
      "|most other parts of the world at no cost and with almost no restrictions|\n",
      "|whatsoever. You may copy it, give it away or re-use it under the terms  |\n",
      "|of the Project Gutenberg License included with this eBook or online at  |\n",
      "|www.gutenberg.org. If you are not located in the United States, you     |\n",
      "|will have to check the laws of the country where you are located before |\n",
      "|using this eBook.                                                       |\n",
      "|                                                                        |\n",
      "+------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "book.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[most, other, par...|\n",
      "|[whatsoever., You...|\n",
      "|[of, the, Project...|\n",
      "|[www.gutenberg.or...|\n",
      "|[will, have, to, ...|\n",
      "|[using, this, eBo...|\n",
      "|                  []|\n",
      "|[Title:, Pride, a...|\n",
      "|                  []|\n",
      "|[Author:, Jane, A...|\n",
      "|                  []|\n",
      "|[Release, Date:, ...|\n",
      "|                  []|\n",
      "|[Language:, English]|\n",
      "|                  []|\n",
      "|[Produced, by:, C...|\n",
      "|[, , , , , , , , ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     eBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.select(F.explode(F.col('line')).alias('word')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.select(F.explode(F.col('line')).alias('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     eBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|     pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      jane|\n",
      "|    austen|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# words.select(F.lower(F.col('word')).alias('word_lower')).show()\n",
    "words.select(F.lower(F.col(\"word\")).alias(\"word_lower\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_lower.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_clean.filter(F.col('word') != '').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nonull = words_clean.filter(F.col('word') != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   word|\n",
      "+-------+\n",
      "|    the|\n",
      "|project|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonull.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = words_nonull.groupby(F.col('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0xffff752db590>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|     online|    5|\n",
      "|       hope|  126|\n",
      "| palpitated|    1|\n",
      "|    solaced|    1|\n",
      "|    elevate|    1|\n",
      "|  solemnity|    5|\n",
      "|     spared|    9|\n",
      "|    courted|    2|\n",
      "|ingratitude|    1|\n",
      "|      parts|    8|\n",
      "|   positive|    4|\n",
      "|    highest|   10|\n",
      "|      hurry|   11|\n",
      "|      oddly|    1|\n",
      "|   laughing|   10|\n",
      "|     speedy|    2|\n",
      "|   slightly|    4|\n",
      "|      scorn|    1|\n",
      "|      staff|    1|\n",
      "|    explain|    7|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "groups.count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = groups.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|     online|    5|\n",
      "|       hope|  126|\n",
      "| palpitated|    1|\n",
      "|    solaced|    1|\n",
      "|    elevate|    1|\n",
      "|  solemnity|    5|\n",
      "|     spared|    9|\n",
      "|    courted|    2|\n",
      "|ingratitude|    1|\n",
      "|      parts|    8|\n",
      "|   positive|    4|\n",
      "|    highest|   10|\n",
      "|      hurry|   11|\n",
      "|      oddly|    1|\n",
      "|   laughing|   10|\n",
      "|     speedy|    2|\n",
      "|   slightly|    4|\n",
      "|      scorn|    1|\n",
      "|      staff|    1|\n",
      "|    explain|    7|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4803|\n",
      "|  to| 4374|\n",
      "|  of| 3949|\n",
      "| and| 3685|\n",
      "| her| 2254|\n",
      "|   a| 2063|\n",
      "|  in| 2024|\n",
      "| was| 1870|\n",
      "|   i| 1778|\n",
      "| she| 1703|\n",
      "|that| 1540|\n",
      "|  it| 1533|\n",
      "| not| 1505|\n",
      "| you| 1317|\n",
      "|  he| 1316|\n",
      "| his| 1278|\n",
      "|  be| 1278|\n",
      "|  as| 1223|\n",
      "| had| 1179|\n",
      "|with| 1140|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.orderBy(F.col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = results.orderBy(F.col('count').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4803|\n",
      "|  to| 4374|\n",
      "|  of| 3949|\n",
      "| and| 3685|\n",
      "| her| 2254|\n",
      "|   a| 2063|\n",
      "|  in| 2024|\n",
      "| was| 1870|\n",
      "|   i| 1778|\n",
      "| she| 1703|\n",
      "|that| 1540|\n",
      "|  it| 1533|\n",
      "| not| 1505|\n",
      "| you| 1317|\n",
      "|  he| 1316|\n",
      "| his| 1278|\n",
      "|  be| 1278|\n",
      "|  as| 1223|\n",
      "| had| 1179|\n",
      "|with| 1140|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_PATH = 's3://data-engg-suman/processed_data/book-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results_2.coalesce(1).write.csv(PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 's3://data-engg-suman/processed_data/book-1/part-00000-bb3b130d-9ec8-47c9-8727-9475ed33fee0-c000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_verify = spark.read.csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "| _c0| _c1|\n",
      "+----+----+\n",
      "| the|4803|\n",
      "|  to|4374|\n",
      "|  of|3949|\n",
      "| and|3685|\n",
      "| her|2254|\n",
      "|   a|2063|\n",
      "|  in|2024|\n",
      "| was|1870|\n",
      "|   i|1778|\n",
      "| she|1703|\n",
      "|that|1540|\n",
      "|  it|1533|\n",
      "| not|1505|\n",
      "| you|1317|\n",
      "|  he|1316|\n",
      "| his|1278|\n",
      "|  be|1278|\n",
      "|  as|1223|\n",
      "| had|1179|\n",
      "|with|1140|\n",
      "+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_2_verify.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .appName(\"Demo App\")\n",
    "            .config(\"spark.ui.port\", \"4050\")\n",
    "            .getOrCreate()\n",
    "            )\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Given the following data frame, programmatically count the number of columns that\n",
    "aren’t strings (answer = only one column isn’t a string).\n",
    "createDataFrame() allows you to create a data frame from a variety of sources,\n",
    "such as a pandas data frame or (in this case) a list of lists.\n",
    "exo2_2_df = spark.createDataFrame(\n",
    "[[\"test\", \"more test\", 10_000_000_000]], [\"one\", \"two\", \"three\"]\n",
    ")\n",
    "exo2_2_df.printSchema()\n",
    "# root\n",
    "# |-- one: string (nullable = true)\n",
    "# |-- two: string (nullable = true)\n",
    "# |-- three: long (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo2_2_df = spark.createDataFrame(\n",
    "                                [[\"test\", \"more test\", 10_000_000_000]], [\"one\", \"two\", \"three\"]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+\n",
      "| one|      two|      three|\n",
      "+----+---------+-----------+\n",
      "|test|more test|10000000000|\n",
      "+----+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exo2_2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 'string'), ('two', 'string'), ('three', 'bigint')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exo2_2_df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "count = sum([ 1 for i in exo2_2_df.dtypes if i[1] != 'string'])\n",
    "print(count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Rewrite the following code snippet, removing the withColumnRenamed method. Which\n",
    "version is clearer and easier to read?\n",
    "\n",
    "from pyspark.sql.functions import col, length\n",
    "# The `length` function returns the number of characters in a string column.\n",
    "exo2_3_df = (\n",
    "spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    ".select(length(col(\"value\")))\n",
    ".withColumnRenamed(\"length(value)\", \"number_of_char\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|number_of_char|\n",
      "+--------------+\n",
      "|            66|\n",
      "|             0|\n",
      "|            64|\n",
      "|            68|\n",
      "|            67|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 's3://data-engg-suman/dataset/all-book-sataset/gutenberg_books/1342-0.txt'\n",
    "\n",
    "exo2_3_df = ( spark\n",
    "                .read\n",
    "                .text(DATA_PATH)\n",
    "                .select(F.length(F.col('value')))\n",
    "                .withColumnRenamed(\"length(value)\", \"number_of_char\")\n",
    ") \n",
    "\n",
    "exo2_3_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|number_of_char|\n",
      "+--------------+\n",
      "|            66|\n",
      "|             0|\n",
      "|            64|\n",
      "|            68|\n",
      "|            67|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 's3://data-engg-suman/dataset/all-book-sataset/gutenberg_books/1342-0.txt'\n",
    "\n",
    "exo2_3_df = ( spark\n",
    "                .read\n",
    "                .text(DATA_PATH)\n",
    "                .select(F.length(F.col('value')).alias('number_of_char'))\n",
    ") \n",
    "\n",
    "exo2_3_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assume a data frame exo2_4_df. The following code block gives an error. What is the\n",
    "problem, and how can you solve it?\n",
    "from pyspark.sql.functions import col, greatest\n",
    "exo2_4_df = spark.createDataFrame(\n",
    "[[\"key\", 10_000, 20_000]], [\"key\", \"value1\", \"value2\"]\n",
    ")\n",
    "exo2_4_df.printSchema()\n",
    "# root\n",
    "# |-- key: string (containsNull = true)\n",
    "# |-- value1: long (containsNull = true)\n",
    "# |-- value2: long (containsNull = true)\n",
    "# `greatest` will return the greatest value of the list of column names,\n",
    "# skipping null value\n",
    "# The following statement will return an error\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "try:\n",
    "exo2_4_mod = exo2_4_df.select(\n",
    "greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\n",
    ").select(\"key\", \"max_value\")\n",
    "except AnalysisException as err:\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .appName(\"Demo App\")\n",
    "            .config(\"spark.ui.port\", \"4050\")\n",
    "            .getOrCreate()\n",
    "            )\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, greatest\n",
    "exo2_4_df = spark.createDataFrame(\n",
    "[[\"key\", 10_000, 20_000]], [\"key\", \"value1\", \"value2\"]\n",
    ")\n",
    "exo2_4_df.printSchema()\n",
    "# root\n",
    "# |-- key: string (containsNull = true)\n",
    "# |-- value1: long (containsNull = true)\n",
    "# |-- value2: long (containsNull = true)\n",
    "# `greatest` will return the greatest value of the list of column names,\n",
    "# skipping null value\n",
    "# The following statement will return an error\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "try:\n",
    "exo2_4_mod = exo2_4_df.select(\n",
    "greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\n",
    ").select(\"key\", \"max_value\")\n",
    "except AnalysisException as err:\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo2_4_df = spark.createDataFrame(\n",
    "[[\"key\", 10_000, 20_000]], [\"key\", \"value1\", \"value2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key', 'value1', 'value2']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exo2_4_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|key|value1|value2|\n",
      "+---+------+------+\n",
      "|key| 10000| 20000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exo2_4_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'key' does not exist. Did you mean one of the following? [maximum_value];\n",
      "'Project ['key, 'max_value]\n",
      "+- Project [greatest(value1#39L, value2#40L) AS maximum_value#100L]\n",
      "   +- LogicalRDD [key#38, value1#39L, value2#40L], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F \n",
    "\n",
    "try:\n",
    "    exo2_4_mod = exo2_4_df.select(\n",
    "                                    F.greatest(F.col(\"value1\"), F.col(\"value2\")).alias(\"maximum_value\")\n",
    "                                    ).select(\"key\", \"max_value\")\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|maximum_value|key|\n",
      "+-------------+---+\n",
      "|        20000|key|\n",
      "+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F \n",
    "\n",
    "try:\n",
    "    exo2_4_mod = exo2_4_df.select((F.greatest(F.col(\"value1\"), F.col(\"value2\"))).alias(\"maximum_value\"), F.col('key'))\n",
    "except AnalysisException as err:\n",
    "    print(err)\n",
    "\n",
    "exo2_4_mod.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take our words_nonull data frame, available in the next listing.\n",
    "\n",
    "a) Remove all of the occurrences of the word is.\n",
    "\n",
    "b) (Challenge) Using the length function, keep only the words with more than three\n",
    "characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt' \n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127438"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_nonull.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "921"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_nonull.filter(F.col('word') == 'is').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126517"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_without_is = words_nonull.filter(F.col('word') != 'is')\n",
    "words_without_is.count() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt' \n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')\n",
    "\n",
    "words_without_is = words_nonull.filter(F.col('word') != 'is')\n",
    "\n",
    "words_without_is.filter(F.col('word') == 'is')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|    pride|\n",
      "|prejudice|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "|   united|\n",
      "|   states|\n",
      "|     most|\n",
      "|    other|\n",
      "|    parts|\n",
      "|    world|\n",
      "|     cost|\n",
      "|     with|\n",
      "|   almost|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt' \n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')\n",
    "\n",
    "words_len_more_3 = words_nonull.filter(F.length(F.col('word')) > 3)\n",
    "\n",
    "words_len_more_3.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The where clause takes a Boolean expression over one or many columns to filter the\n",
    "data frame. Beyond the usual Boolean operators (>, <, ==, <=, >=, !=), PySpark provides\n",
    "other functions that return Boolean columns in the pyspark.sql.functions\n",
    "module.\n",
    "A good example is the isin() method (applied on a Column object, like\n",
    "col(…).isin(…)), which takes a list of values as a parameter, and will return only the\n",
    "records where the value in the column equals a member of the list.\n",
    "Let’s say you want to remove the words is, not, the and if from your list of words,\n",
    "using a single where() method on the words_nonull data frame. Write the code to\n",
    "do so."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt' \n",
    "stopping_words = \"is not the if\".split(\" \")\n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')\n",
    "\n",
    "words_without_stopping_words = words_nonull.select(F.col('word')).where(~F.col('word').isin(stopping_words))\n",
    "words_without_stopping_words.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.7"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "One of your friends comes to you with the following code. They have no idea why it\n",
    "doesn’t work. Can you diagnose the problem in the try block, explain why it is an\n",
    "error, and provide a fix?\n",
    "\n",
    "from pyspark.sql.functions import col, split\n",
    "try:\n",
    "book = spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    "book = book.printSchema()\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "except AnalysisException as err:\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51267/3844583780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mAnalysisException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql import functions as F \n",
    "\n",
    "DATASET_PATH = 's3://data-engg-suman/dataset/all-book-sataset/gutenberg_books/1342-0.txt' \n",
    " \n",
    "try:\n",
    "    book = spark.read.text(DATASET_PATH)\n",
    "    book = book.printSchema()\n",
    "    lines = book.select(F.split(book.value, \" \").alias(\"line\"))\n",
    "    words = lines.select(F.explode(col(\"line\")).alias(\"word\"))\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql import functions as F \n",
    "\n",
    "DATASET_PATH = 's3://data-engg-suman/dataset/all-book-sataset/gutenberg_books/1342-0.txt' \n",
    " \n",
    "try:\n",
    "    book = spark.read.text(DATASET_PATH)\n",
    "    # book = book.printSchema()\n",
    "    lines = book.select(F.split(book.value, \" \").alias(\"line\"))\n",
    "    words = lines.select(F.explode(col(\"line\")).alias(\"word\"))\n",
    "except AnalysisException as err:\n",
    "    print(err)\n",
    "\n",
    "words.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Starting with the word_nonull seen in this section, which of the following expressions\n",
    "would return the number of words per letter count (e.g., there are X one-letter\n",
    "words, Y two-letter words, etc.)?\n",
    "Assume that pyspark.sql.functions.col, pyspark.sql.functions.length are\n",
    "imported.\n",
    "a words_nonull.select(length(col(\"word\"))).groupby(\"length\").count()\n",
    "b words_nonull.select(length(col(\"word\")).alias(\"length\")).groupby\n",
    "(\"length\").count()\n",
    "c words_nonull.groupby(\"length\").select(\"length\").count()\n",
    "d None of those options would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonull.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|    12|  852|\n",
      "|     1| 3879|\n",
      "|     6| 9629|\n",
      "|     3|29354|\n",
      "|     4|22598|\n",
      "|     8| 5326|\n",
      "|    11| 1443|\n",
      "|    14|  122|\n",
      "|    16|    6|\n",
      "|     5|12417|\n",
      "|    15|   35|\n",
      "|     2|24356|\n",
      "|    13|  396|\n",
      "|    17|    3|\n",
      "|     7| 9090|\n",
      "|     9| 5357|\n",
      "|    10| 2575|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Option b \n",
    "words_nonull.select(F.length(F.col('word')).alias('length')).groupBy(F.col('length')).count().show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import functions as F \n",
    "\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .appName(\"Demo App\")\n",
    "            .config(\"spark.ui.port\", \"4050\")\n",
    "            .getOrCreate()\n",
    "            )\n",
    "\n",
    "# Setting the log level to ERROR \n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "\n",
    "# Dataset for all files inside the dataset folder \n",
    "DATASET_PATH = 's3://data-engg-suman/dataset/all-book-sataset/gutenberg_books/*.txt' \n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')\n",
    "\n",
    "groups = words_nonull.groupby(F.col('word'))\n",
    "results = groups.count()\n",
    "results_2 = results.orderBy(F.col('count').desc())\n",
    "\n",
    "PROCESSED_DATA_PATH = 's3://data-engg-suman/processed_data/book-all-book'\n",
    "results_2.coalesce(1).write.csv(PROCESSED_DATA_PATH)\n",
    "\n",
    "# Show\n",
    "results_2.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By modifying the word_count_submit.py program, return the number of distinct\n",
    "words in Jane Austen’s Pride and Prejudice. (Hint: results contains one\n",
    "record for each unique word.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6893 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Dataset for all files inside the dataset folder \n",
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt' \n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')\n",
    "\n",
    "groups = words_nonull.groupby(F.col('word'))\n",
    "results = groups.count()\n",
    "\n",
    "final_result = results.count()\n",
    "\n",
    "print(final_result, type(final_result)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Taking word_count_submit.py, modify the script to return a sample of five words that\n",
    "appear only once in Jane Austen’s Pride and Prejudice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for all files inside the dataset folder \n",
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt' \n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')\n",
    "\n",
    "groups = words_nonull.groupby(F.col('word'))\n",
    "results = groups.count()\n",
    "\n",
    "unique_words = results.orderBy('count').select(F.col('word')).where(F.col('count') == 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Using the substring function (refer to PySpark’s API or the pyspark shell if\n",
    "needed), return the top five most popular first letters (keep only the first letter\n",
    "of each word).\n",
    "2 Compute the number of words starting with a consonant or a vowel. (Hint: The\n",
    "isin() function might be useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for all files inside the dataset folder \n",
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt' \n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')\n",
    "\n",
    "groups = words_nonull.groupby(F.col('word'))\n",
    "results = groups.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|     online|    5|\n",
      "|       hope|  126|\n",
      "| palpitated|    1|\n",
      "|    solaced|    1|\n",
      "|    elevate|    1|\n",
      "|  solemnity|    5|\n",
      "|     spared|    9|\n",
      "|    courted|    2|\n",
      "|ingratitude|    1|\n",
      "|      parts|    8|\n",
      "|   positive|    4|\n",
      "|    highest|   10|\n",
      "|      hurry|   11|\n",
      "|      oddly|    1|\n",
      "|   laughing|   10|\n",
      "|     speedy|    2|\n",
      "|   slightly|    4|\n",
      "|      scorn|    1|\n",
      "|      staff|    1|\n",
      "|    explain|    7|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+\n",
      "|      word|count|first_letter|\n",
      "+----------+-----+------------+\n",
      "|    online|    5|           o|\n",
      "|      hope|  126|           h|\n",
      "|palpitated|    1|           p|\n",
      "|   solaced|    1|           s|\n",
      "|   elevate|    1|           e|\n",
      "+----------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.withColumn('first_letter', F.substring(F.col('word'), 1, 1)).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|first_letter|sum(count)|\n",
      "+------------+----------+\n",
      "|           t|     16844|\n",
      "|           a|     14358|\n",
      "|           h|     10734|\n",
      "|           w|      9388|\n",
      "|           s|      9073|\n",
      "|           i|      8570|\n",
      "|           o|      7596|\n",
      "|           m|      6886|\n",
      "|           b|      6277|\n",
      "|           c|      5104|\n",
      "|           f|      4419|\n",
      "|           d|      4019|\n",
      "|           n|      3579|\n",
      "|           e|      3524|\n",
      "|           p|      3427|\n",
      "|           l|      3239|\n",
      "|           r|      2552|\n",
      "|           y|      2203|\n",
      "|           g|      2018|\n",
      "|           u|      1105|\n",
      "+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution of Part 1 \n",
    "\n",
    "# Dataset for all files inside the dataset folder \n",
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt' \n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')\n",
    "\n",
    "groups = words_nonull.groupby(F.col('word'))\n",
    "results = groups.count()\n",
    "\n",
    "\n",
    "results.withColumn('first_letter', F.substring(F.col('word'), 1, 1)).groupby(F.col('first_letter')).sum().orderBy('sum(count)', ascending=False).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|first_letter|count|\n",
      "+------------+-----+\n",
      "|           a|14358|\n",
      "|           i| 8570|\n",
      "|           u| 1105|\n",
      "|           e| 3524|\n",
      "|           o| 7596|\n",
      "+------------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|first_letter|count|\n",
      "+------------+-----+\n",
      "|           l| 3239|\n",
      "|           p| 3427|\n",
      "|           d| 4019|\n",
      "|           j|  588|\n",
      "|           s| 9073|\n",
      "|           f| 4419|\n",
      "|           h|10734|\n",
      "|           y| 2203|\n",
      "|           q|  259|\n",
      "|           b| 6277|\n",
      "|           g| 2018|\n",
      "|           k|  696|\n",
      "|           v|  918|\n",
      "|           r| 2552|\n",
      "|           m| 6886|\n",
      "|           n| 3579|\n",
      "|           c| 5104|\n",
      "|           t|16844|\n",
      "|           x|   62|\n",
      "|           w| 9388|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution of Part 2 \n",
    "\n",
    "# Dataset for all files inside the dataset folder \n",
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt' \n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')\n",
    "\n",
    "vowel = list('aeiou')\n",
    "\n",
    "results_with_first_letter = words_nonull.withColumn('first_letter', F.substring(F.col('word'), 1, 1))\n",
    "\n",
    "words_starting_WITH_vowel = results_with_first_letter.select(F.col('word'), F.col('first_letter')).where(F.col('first_letter').isin(vowel)).groupby(F.col('first_letter')).count()\n",
    "words_starting_WITH_vowel.show()\n",
    "\n",
    "words_starting_NOT_WITH_vowel = results_with_first_letter.select(F.col('word'), F.col('first_letter')).where(~F.col('first_letter').isin(vowel)).groupby(F.col('first_letter')).count()\n",
    "words_starting_NOT_WITH_vowel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|first_letter_vowel|sum(count)|\n",
      "+------------------+----------+\n",
      "|              true|     35153|\n",
      "|             false|     92285|\n",
      "+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset for all files inside the dataset folder \n",
    "DATASET_PATH = 's3://data-engg-suman/dataset/book-1.txt' \n",
    "\n",
    "book = spark.read.text(DATASET_PATH)\n",
    "lines = book.select(F.split(book.value, ' ').alias('line'))\n",
    "words = lines.select(F.explode(F.col('line')).alias('word'))\n",
    "words_lower = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(F.regexp_extract(F.col('word_lower'), '[a-z]*', 0).alias('word'))\n",
    "words_nonull = words_clean.filter(F.col('word') != '')\n",
    "\n",
    "groups = words_nonull.groupby(F.col('word'))\n",
    "results = groups.count()\n",
    "\n",
    "results.withColumn('first_letter_vowel', F.substring(F.col('word'), 1, 1).isin(vowel)).groupby(F.col('first_letter_vowel')).sum().show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
