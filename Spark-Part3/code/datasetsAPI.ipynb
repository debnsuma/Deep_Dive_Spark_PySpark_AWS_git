{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a SPARK Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"My Application\")\n",
    "         .config(\"spark.ui.port\", \"4050\")\n",
    "         .getOrCreate()\n",
    "         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = \"s3://data-engg-suman/dataset/orders.csv\"\n",
    "\n",
    "# Load the data (Read is an ACTION)\n",
    "ordersdf = (spark\n",
    "            .read\n",
    "            .option(\"header\", True)       # for headers (1st line of the dataset)\n",
    "            .option(\"inferSchema\", True)  # for datatypes\n",
    "            .csv(dataset_path)\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 lines \n",
    "ordersdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema \n",
    "ordersdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedOrderdf = (ordersdf.where(\"order_customer_id > 1000\")              # This is NOT an action, like filer in RDD \n",
    "                          .select(\"order_id\", \"order_customer_id\")        # This is NOT an action, like map in RDD\n",
    "                          .groupBy(\"order_customer_id\")                   # This is NOT an action, like groupByKey/reduceByKey in RDD\n",
    "                          .count()                                        # This is NOT an action, like count in RDD \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|order_customer_id|count|\n",
      "+-----------------+-----+\n",
      "|             1088|    4|\n",
      "|            12046|    4|\n",
      "|             6357|    7|\n",
      "|             4101|    5|\n",
      "|             9465|    7|\n",
      "|             2122|    2|\n",
      "|            10206|    4|\n",
      "|            10817|    6|\n",
      "|             3918|    7|\n",
      "|            11141|    7|\n",
      "|             1591|    6|\n",
      "|            10623|    8|\n",
      "|             2366|    4|\n",
      "|             4519|    3|\n",
      "|            11317|    7|\n",
      "|             1342|    3|\n",
      "|             8638|    8|\n",
      "|             9852|    2|\n",
      "|            10362|    6|\n",
      "|             7880|    5|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupedOrderdf.show()  # This is an action, by default shows 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|order_customer_id|count|\n",
      "+-----------------+-----+\n",
      "|             1088|    4|\n",
      "|            12046|    4|\n",
      "|             6357|    7|\n",
      "|             4101|    5|\n",
      "|             9465|    7|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupedOrderdf.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "my_conf = SparkConf()\n",
    "my_conf.set(\"spark.app.name\", \"Problem 1\")\n",
    "my_conf.set(\"spark.ui.port\", \"4050\")\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=my_conf) \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Set the logging level to `error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Using the standard dataframe reader API load the file and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, FloatType\n",
    "\n",
    "window_data_schema = StructType(\n",
    "    [\n",
    "        StructField('country', StringType()),\n",
    "        StructField('weeknum', IntegerType()),\n",
    "        StructField('numinvoices', IntegerType()),\n",
    "        StructField('totalquantity', IntegerType()), \n",
    "        StructField('invoicevalue', FloatType())  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read the data from the CSV \n",
    "DATASET_PATH = \"s3://data-engg-suman/dataset/windowdata.csv\"\n",
    "\n",
    "windows_data_df = spark \\\n",
    "                    .read \\\n",
    "                    .format('csv') \\\n",
    "                    .option('header', True) \\\n",
    "                    .schema(window_data_schema) \\\n",
    "                    .option('path', DATASET_PATH) \\\n",
    "                    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-----------+-------------+------------+\n",
      "|       country|weeknum|numinvoices|totalquantity|invoicevalue|\n",
      "+--------------+-------+-----------+-------------+------------+\n",
      "|       Germany|     48|         11|         1795|     3309.75|\n",
      "|     Lithuania|     48|          3|          622|     1598.06|\n",
      "|       Germany|     49|         12|         1852|     4521.39|\n",
      "|       Bahrain|     51|          1|           54|      205.74|\n",
      "|       Iceland|     49|          1|          319|      711.79|\n",
      "|         India|     51|          5|           95|      276.84|\n",
      "|     Australia|     50|          2|          133|      387.95|\n",
      "|         Italy|     49|          1|           -2|       -17.0|\n",
      "|         India|     49|          5|         1280|      3284.1|\n",
      "|         Spain|     50|          2|          400|     1049.01|\n",
      "|United Kingdom|     51|        200|        28782|    75103.46|\n",
      "|        Norway|     49|          1|         1730|     1867.98|\n",
      "|United Kingdom|     48|        478|        68865|   166116.72|\n",
      "|        France|     51|          5|          847|     1702.87|\n",
      "|      Portugal|     49|          4|          726|     1844.67|\n",
      "|         Spain|     48|          1|          400|       620.0|\n",
      "|         India|     48|          7|         2822|     3147.23|\n",
      "|       Germany|     50|         15|         1973|     5065.79|\n",
      "|         Italy|     51|          1|          131|       383.7|\n",
      "|        France|     49|          9|         2303|     4527.01|\n",
      "+--------------+-------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windows_data_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Use the standard dataframe `writer` api to save it in `parquet` format. \n",
    "While saving make sure data is stored where we should have a folder for each country, weeknum (combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "\n",
    "OUTPUT_PATH = \"s3://data-engg-suman/processed_data\"\n",
    "OUTPUT_FILE_PATH = os.path.join(OUTPUT_PATH, 'windows_data')\n",
    "\n",
    "windows_data_df \\\n",
    "    .write \\\n",
    "    .partitionBy('country', 'weeknum') \\\n",
    "    .parquet(path=OUTPUT_FILE_PATH, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_data_parquat_df = spark \\\n",
    "                            .read \\\n",
    "                            .format('parquet') \\\n",
    "                            .option('path', OUTPUT_FILE_PATH) \\\n",
    "                            .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+---------------+-------+\n",
      "|numinvoices|totalquantity|invoicevalue|        country|weeknum|\n",
      "+-----------+-------------+------------+---------------+-------+\n",
      "|          1|          107|      358.25|      Australia|     48|\n",
      "|          1|          196|      320.08|          Japan|     48|\n",
      "|         15|         1973|     5065.79|        Germany|     50|\n",
      "|          3|          622|     1598.06|      Lithuania|     48|\n",
      "|          4|          726|     1844.67|       Portugal|     49|\n",
      "|          2|          133|      387.95|      Australia|     50|\n",
      "|          2|            3|      257.04|        Austria|     50|\n",
      "|          1|          164|       427.8|          Italy|     48|\n",
      "|          2|         3897|     7384.99|          Japan|     49|\n",
      "|          1|         1254|       892.8|        Finland|     50|\n",
      "|          1|           -2|       -17.0|          Italy|     49|\n",
      "|          4|         1299|     2808.16|         France|     48|\n",
      "|          1|          214|       258.9|      Australia|     49|\n",
      "|        200|        28782|    75103.46| United Kingdom|     51|\n",
      "|        677|       105498|   244020.45| United Kingdom|     49|\n",
      "|         11|         1795|     3309.75|        Germany|     48|\n",
      "|          1|           80|      363.53|Channel Islands|     49|\n",
      "|          1|          110|       303.4|    Switzerland|     48|\n",
      "|          7|         2822|     3147.23|          India|     48|\n",
      "|          2|          942|      838.65|        Belgium|     51|\n",
      "+-----------+-------------+------------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "windows_data_parquat_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-----------+-------------+------------+\n",
      "|       country|weeknum|numinvoices|totalquantity|invoicevalue|\n",
      "+--------------+-------+-----------+-------------+------------+\n",
      "|       Germany|     48|         11|         1795|     3309.75|\n",
      "|     Lithuania|     48|          3|          622|     1598.06|\n",
      "|       Germany|     49|         12|         1852|     4521.39|\n",
      "|       Bahrain|     51|          1|           54|      205.74|\n",
      "|       Iceland|     49|          1|          319|      711.79|\n",
      "|         India|     51|          5|           95|      276.84|\n",
      "|     Australia|     50|          2|          133|      387.95|\n",
      "|         Italy|     49|          1|           -2|       -17.0|\n",
      "|         India|     49|          5|         1280|      3284.1|\n",
      "|         Spain|     50|          2|          400|     1049.01|\n",
      "|United Kingdom|     51|        200|        28782|    75103.46|\n",
      "|        Norway|     49|          1|         1730|     1867.98|\n",
      "|United Kingdom|     48|        478|        68865|   166116.72|\n",
      "|        France|     51|          5|          847|     1702.87|\n",
      "|      Portugal|     49|          4|          726|     1844.67|\n",
      "|         Spain|     48|          1|          400|       620.0|\n",
      "|         India|     48|          7|         2822|     3147.23|\n",
      "|       Germany|     50|         15|         1973|     5065.79|\n",
      "|         Italy|     51|          1|          131|       383.7|\n",
      "|        France|     49|          9|         2303|     4527.01|\n",
      "+--------------+-------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windows_data_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Also use the dataframe write api to save the data in Avro format. \n",
    "While saving make sure data is stored where we should have a folder for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ending the previous Spark Session \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "my_conf = SparkConf()\n",
    "my_conf.set(\"spark.app.name\", \"Problem 1 (Avro)\")\n",
    "my_conf.set(\"spark.ui.port\", \"4050\")\n",
    "my_conf.set(\"spark.jars\", \"s3://data-engg-suman/bin/spark-avro_2.11-2.4.4.jar\")\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=my_conf) \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the data in the DF \n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, FloatType\n",
    "\n",
    "window_data_schema = StructType(\n",
    "    [\n",
    "        StructField('country', StringType()),\n",
    "        StructField('weeknum', IntegerType()),\n",
    "        StructField('numinvoices', IntegerType()),\n",
    "        StructField('totalquantity', IntegerType()), \n",
    "        StructField('invoicevalue', FloatType())  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read the data from the CSV \n",
    "DATASET_PATH = \"s3://data-engg-suman/dataset/windowdata.csv\"\n",
    "\n",
    "windows_data_df = spark \\\n",
    "                    .read \\\n",
    "                    .format('csv') \\\n",
    "                    .option('header', True) \\\n",
    "                    .schema(window_data_schema) \\\n",
    "                    .option('path', DATASET_PATH) \\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": " Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8195/2212547728.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_FILE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m:  Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".        "
     ]
    }
   ],
   "source": [
    "# Now writing the data back in AVRO format \n",
    "\n",
    "import os \n",
    "\n",
    "OUTPUT_PATH = \"s3://data-engg-suman/processed_data\"\n",
    "OUTPUT_FILE_PATH = os.path.join(OUTPUT_PATH, 'windows_data_avro')\n",
    "    \n",
    "windows_data_df \\\n",
    "    .write \\\n",
    "    .format(\"avro\") \\\n",
    "    .mode('Overwrite') \\\n",
    "    .option('path', OUTPUT_FILE_PATH) \\\n",
    "    .save() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0+amzn.0'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "my_conf = SparkConf()\n",
    "my_conf.set(\"spark.app.name\", \"Problem 2\")\n",
    "my_conf.set(\"spark.ui.port\", \"4050\")\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=my_conf) \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set the logging level to error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the data file windowdata.csv as a rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the CSV \n",
    "DATASET_PATH = \"s3://data-engg-suman/dataset/windowdata.csv\"\n",
    "\n",
    "rdd = spark.sparkContext.textFile(DATASET_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spain,49,1,67,174.72']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create a dataframe from this RDD by defining case class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, FloatType\n",
    "\n",
    "window_data_schema = StructType(\n",
    "    [\n",
    "        StructField('country', StringType()),\n",
    "        StructField('weeknum', IntegerType()),\n",
    "        StructField('numinvoices', IntegerType()),\n",
    "        StructField('totalquantity', IntegerType()), \n",
    "        StructField('invoicevalue', FloatType())  \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(line):\n",
    "    _ = line.split(',')\n",
    "    return str(_[0]), int(_[1]), int(_[2]), int(_[3]), float(_[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spain', 49, 1, 67, 174.72)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(parser).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = rdd.map(parser).toDF(window_data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-----------+-------------+------------+\n",
      "|       country|weeknum|numinvoices|totalquantity|invoicevalue|\n",
      "+--------------+-------+-----------+-------------+------------+\n",
      "|         Spain|     49|          1|           67|      174.72|\n",
      "|       Germany|     48|         11|         1795|     3309.75|\n",
      "|     Lithuania|     48|          3|          622|     1598.06|\n",
      "|       Germany|     49|         12|         1852|     4521.39|\n",
      "|       Bahrain|     51|          1|           54|      205.74|\n",
      "|       Iceland|     49|          1|          319|      711.79|\n",
      "|         India|     51|          5|           95|      276.84|\n",
      "|     Australia|     50|          2|          133|      387.95|\n",
      "|         Italy|     49|          1|           -2|       -17.0|\n",
      "|         India|     49|          5|         1280|      3284.1|\n",
      "|         Spain|     50|          2|          400|     1049.01|\n",
      "|United Kingdom|     51|        200|        28782|    75103.46|\n",
      "|        Norway|     49|          1|         1730|     1867.98|\n",
      "|United Kingdom|     48|        478|        68865|   166116.72|\n",
      "|        France|     51|          5|          847|     1702.87|\n",
      "|      Portugal|     49|          4|          726|     1844.67|\n",
      "|         Spain|     48|          1|          400|       620.0|\n",
      "|         India|     48|          7|         2822|     3147.23|\n",
      "|       Germany|     50|         15|         1973|     5065.79|\n",
      "|         Italy|     51|          1|          131|       383.7|\n",
      "+--------------+-------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save this dataframe in JSON format in 8 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"s3://data-engg-suman/processed_data/windowdata.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write \\\n",
    "    .format('json') \\\n",
    "    .mode('Overwrite') \\\n",
    "    .option('path', DATASET_PATH) \\\n",
    "    .save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
