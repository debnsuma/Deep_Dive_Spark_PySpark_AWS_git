{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "my_conf = SparkConf()\n",
    "my_conf.set(\"spark.app.name\", \"My Application\")\n",
    "my_conf.set(\"spark.ui.port\", \"4050\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=my_conf) \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the CSV \n",
    "DATASET_PATH = \"s3://data-engg-suman/dataset/orders.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the data in the DF \n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, FloatType, DateType\n",
    "\n",
    "orders_data_schema = StructType(\n",
    "    [\n",
    "        StructField('order_id', IntegerType()),\n",
    "        StructField('order_date', DateType()),\n",
    "        StructField('order_customer_id', IntegerType()),\n",
    "        StructField('order_status', StringType()), \n",
    "    ]\n",
    ")\n",
    "\n",
    "orders_data_df = spark \\\n",
    "                    .read \\\n",
    "                    .format('csv') \\\n",
    "                    .option('header', True) \\\n",
    "                    .schema(orders_data_schema) \\\n",
    "                    .option('path', DATASET_PATH) \\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use the writer API to SINK the data \n",
    "import os \n",
    "\n",
    "OUTPUT_PATH = \"s3://data-engg-suman/processed_data2\"\n",
    "OUTPUT_FILE_PATH = os.path.join(OUTPUT_PATH, 'orders_data')\n",
    "\n",
    "orders_data_df \\\n",
    "    .write \\\n",
    "    .format('parquet') \\\n",
    "    .mode('overwrite') \\\n",
    "    .option('path', OUTPUT_FILE_PATH) \\\n",
    "    .save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data written \n",
    "\n",
    "orders_data_df_parquet = spark \\\n",
    "                    .read \\\n",
    "                    .format('parquet') \\\n",
    "                    .option('path', OUTPUT_FILE_PATH) \\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|\n",
      "|       4|2013-07-25|             8827|         CLOSED|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|\n",
      "|       6|2013-07-25|             7130|       COMPLETE|\n",
      "|       7|2013-07-25|             4530|       COMPLETE|\n",
      "|       8|2013-07-25|             2911|     PROCESSING|\n",
      "|       9|2013-07-25|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25|             1837|         CLOSED|\n",
      "|      13|2013-07-25|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25|             9842|     PROCESSING|\n",
      "|      15|2013-07-25|             2568|       COMPLETE|\n",
      "|      16|2013-07-25|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25|             2667|       COMPLETE|\n",
      "|      18|2013-07-25|             1205|         CLOSED|\n",
      "|      19|2013-07-25|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25|             9198|     PROCESSING|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_data_df_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of partitions \n",
    "orders_data_df_parquet.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "repart_orders_data_df = orders_data_df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "repart_orders_data_df \\\n",
    "    .write \\\n",
    "    .format('parquet') \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('order_status') \\\n",
    "    .option('path', OUTPUT_FILE_PATH) \\\n",
    "    .save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repart_orders_data_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "repart_orders_data_df \\\n",
    "    .write \\\n",
    "    .format('parquet') \\\n",
    "    .mode('overwrite') \\\n",
    "    .option('maxrecordsperfile', 20000) \\\n",
    "    .option('path', OUTPUT_FILE_PATH) \\\n",
    "    .save() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_data_df.createOrReplaceTempView('orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|\n",
      "|       4|2013-07-25|             8827|         CLOSED|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|\n",
      "|       6|2013-07-25|             7130|       COMPLETE|\n",
      "|       7|2013-07-25|             4530|       COMPLETE|\n",
      "|       8|2013-07-25|             2911|     PROCESSING|\n",
      "|       9|2013-07-25|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25|             1837|         CLOSED|\n",
      "|      13|2013-07-25|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25|             9842|     PROCESSING|\n",
      "|      15|2013-07-25|             2568|       COMPLETE|\n",
      "|      16|2013-07-25|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25|             2667|       COMPLETE|\n",
      "|      18|2013-07-25|             1205|         CLOSED|\n",
      "|      19|2013-07-25|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25|             9198|     PROCESSING|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM orders\").show() # This table is distributed across machines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|   order_status|count|\n",
      "+---------------+-----+\n",
      "|       COMPLETE|22899|\n",
      "|PENDING_PAYMENT|15030|\n",
      "|     PROCESSING| 8274|\n",
      "|        PENDING| 7609|\n",
      "|         CLOSED| 7556|\n",
      "|        ON_HOLD| 3798|\n",
      "|SUSPECTED_FRAUD| 1558|\n",
      "|       CANCELED| 1428|\n",
      "| PAYMENT_REVIEW|  729|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT order_status, count(*) as count \\\n",
    "            FROM orders \\\n",
    "            GROUP BY order_status \\\n",
    "            ORDER BY count DESC\").show() # This table is distributed across machines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|order_status|count|\n",
      "+------------+-----+\n",
      "|      CLOSED| 7556|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find out how many orders each customer has placed which are in CLOSED state\n",
    "\n",
    "spark.sql(\"SELECT order_status, count(*) as count \\\n",
    "            FROM orders \\\n",
    "            GROUP BY order_status HAVING order_status = 'CLOSED'\\\n",
    "            ORDER BY count DESC\").show() # This table is distributed across machines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|order_customer_id|count|\n",
      "+-----------------+-----+\n",
      "|             1833|    6|\n",
      "|             1687|    5|\n",
      "|             5493|    5|\n",
      "|             1363|    5|\n",
      "|             8974|    4|\n",
      "|             2774|    4|\n",
      "|             2236|    4|\n",
      "|             4282|    4|\n",
      "|             5582|    4|\n",
      "|            12431|    4|\n",
      "|             9740|    4|\n",
      "|             7879|    4|\n",
      "|             4573|    4|\n",
      "|             9213|    4|\n",
      "|             4588|    4|\n",
      "|            10111|    4|\n",
      "|             2768|    4|\n",
      "|             7948|    4|\n",
      "|             9804|    4|\n",
      "|             1521|    4|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT order_customer_id, count(*) as count \\\n",
    "            FROM orders \\\n",
    "            WHERE order_status = 'CLOSED'\\\n",
    "            GROUP BY order_customer_id \\\n",
    "            ORDER BY count DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|   order_status|count|\n",
      "+---------------+-----+\n",
      "|       COMPLETE|22899|\n",
      "|PENDING_PAYMENT|15030|\n",
      "|     PROCESSING| 8274|\n",
      "|        PENDING| 7609|\n",
      "|         CLOSED| 7556|\n",
      "|        ON_HOLD| 3798|\n",
      "|SUSPECTED_FRAUD| 1558|\n",
      "|       CANCELED| 1428|\n",
      "| PAYMENT_REVIEW|  729|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_data_df.createOrReplaceTempView('orders')\n",
    "\n",
    "spark.sql(\"SELECT order_status, count(*) as count \\\n",
    "            FROM orders \\\n",
    "            GROUP BY order_status \\\n",
    "            ORDER BY count DESC\").show() # This table is distributed across machines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data in the form of table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_data_df.write \\\n",
    "            .format('csv') \\\n",
    "            .mode('overwrite') \\\n",
    "            .saveAsTable('retail.orders') \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='orders', database='retail', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='orders', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('retail')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `bucketBy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_data_df.write \\\n",
    "            .format('csv') \\\n",
    "            .mode('overwrite') \\\n",
    "            .bucketBy(3, 'order_customer_id') \\\n",
    "            .sortBy('order_customer_id') \\\n",
    "            .saveAsTable('retail.orders') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the CSV \n",
    "DATASET_PATH = \"s3://data-engg-suman/dataset/orders_new.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 2013-07-25\\t11599,CLOSED',\n",
       " '2 2013-07-25\\t256,PENDING_PAYMENT',\n",
       " '3 2013-07-25\\t12111,COMPLETE',\n",
       " '4 2013-07-25\\t8827,CLOSED',\n",
       " '5 2013-07-25\\t11318,COMPLETE',\n",
       " '6 2013-07-25\\t7130,COMPLETE',\n",
       " '7 2013-07-25\\t4530,COMPLETE',\n",
       " '8 2013-07-25\\t2911,PROCESSING',\n",
       " '9 2013-07-25\\t5657,PENDING_PAYMENT',\n",
       " '10 2013-07-25\\t5648,PENDING_PAYMENT',\n",
       " '11 2013-07-25\\t918,PAYMENT_REVIEW',\n",
       " '12 2013-07-25\\t1837,CLOSED']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_regx = r'^(\\S+) (\\S+)\\t(\\S+)\\,(\\S+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|value                              |\n",
      "+-----------------------------------+\n",
      "|1 2013-07-25\\t11599,CLOSED         |\n",
      "|2 2013-07-25\\t256,PENDING_PAYMENT  |\n",
      "|3 2013-07-25\\t12111,COMPLETE       |\n",
      "|4 2013-07-25\\t8827,CLOSED          |\n",
      "|5 2013-07-25\\t11318,COMPLETE       |\n",
      "|6 2013-07-25\\t7130,COMPLETE        |\n",
      "|7 2013-07-25\\t4530,COMPLETE        |\n",
      "|8 2013-07-25\\t2911,PROCESSING      |\n",
      "|9 2013-07-25\\t5657,PENDING_PAYMENT |\n",
      "|10 2013-07-25\\t5648,PENDING_PAYMENT|\n",
      "|11 2013-07-25\\t918,PAYMENT_REVIEW  |\n",
      "|12 2013-07-25\\t1837,CLOSED         |\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F \n",
    "final_df = df.select(F.regexp_extract('value', my_regx, 1).alias(\"order_id\"), F.regexp_extract('value', my_regx, 2).alias(\"date\"), F.regexp_extract('value', my_regx, 3).alias(\"customer_id\"), F.regexp_extract('value', my_regx, 4).alias(\"status\")  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|order_id|      date|customer_id|         status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|2013-07-25|      11599|         CLOSED|\n",
      "|       2|2013-07-25|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|      12111|       COMPLETE|\n",
      "|       4|2013-07-25|       8827|         CLOSED|\n",
      "|       5|2013-07-25|      11318|       COMPLETE|\n",
      "|       6|2013-07-25|       7130|       COMPLETE|\n",
      "|       7|2013-07-25|       4530|       COMPLETE|\n",
      "|       8|2013-07-25|       2911|     PROCESSING|\n",
      "|       9|2013-07-25|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25|       1837|         CLOSED|\n",
      "+--------+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|order_id|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "|      11|\n",
      "|      12|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.select('order_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Object, Column String and Comumn Expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 's3://data-engg-suman/dataset/orders.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "                    .read \\\n",
    "                    .format('csv') \\\n",
    "                    .option('header', True) \\\n",
    "                    .option('inferSchema', True) \\\n",
    "                    .option('path', DATASET_PATH) \\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column String \n",
    "df.select('order_id', 'order_status').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column Object\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.select( F.col('order_id'), F.column('order_status') ).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column Object and Column String \n",
    "\n",
    "df.select('order_id', F.column('order_status') ).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------+\n",
      "|order_id|new_order_status      |\n",
      "+--------+----------------------+\n",
      "|1       |CLOSED_STATUS         |\n",
      "|2       |PENDING_PAYMENT_STATUS|\n",
      "|3       |COMPLETE_STATUS       |\n",
      "|4       |CLOSED_STATUS         |\n",
      "|5       |COMPLETE_STATUS       |\n",
      "|6       |COMPLETE_STATUS       |\n",
      "|7       |COMPLETE_STATUS       |\n",
      "|8       |PROCESSING_STATUS     |\n",
      "|9       |PENDING_PAYMENT_STATUS|\n",
      "|10      |PENDING_PAYMENT_STATUS|\n",
      "|11      |PAYMENT_REVIEW_STATUS |\n",
      "|12      |CLOSED_STATUS         |\n",
      "|13      |PENDING_PAYMENT_STATUS|\n",
      "|14      |PROCESSING_STATUS     |\n",
      "|15      |COMPLETE_STATUS       |\n",
      "|16      |PENDING_PAYMENT_STATUS|\n",
      "|17      |COMPLETE_STATUS       |\n",
      "|18      |CLOSED_STATUS         |\n",
      "|19      |PENDING_PAYMENT_STATUS|\n",
      "|20      |PROCESSING_STATUS     |\n",
      "+--------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col('order_id'), F.concat(F.col('order_status'), F.lit('_STATUS')).alias('new_order_status')).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Defined Functions using Structured API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 's3://data-engg-suman/dataset/dataset1'\n",
    "\n",
    "df = spark \\\n",
    "        .read \\\n",
    "        .format('csv') \\\n",
    "        .option('inferSchema', True) \\\n",
    "        .option('header', False) \\\n",
    "        .option('path', DATASET_PATH) \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|    _c0|_c1|      _c2|\n",
      "+-------+---+---------+\n",
      "|  sumit| 30|bangalore|\n",
      "|  kapil| 32|hyderabad|\n",
      "|sathish| 16|  chennai|\n",
      "|   ravi| 39|bangalore|\n",
      "| kavita| 12|hyderabad|\n",
      "|  kavya| 19|   mysore|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   name|age|     city|\n",
      "+-------+---+---------+\n",
      "|  sumit| 30|bangalore|\n",
      "|  kapil| 32|hyderabad|\n",
      "|sathish| 16|  chennai|\n",
      "|   ravi| 39|bangalore|\n",
      "| kavita| 12|hyderabad|\n",
      "|  kavya| 19|   mysore|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets add some meaningful coln names \n",
    "\n",
    "df1 = df.toDF('name', 'age', 'city')\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   name|age|     city|\n",
      "+-------+---+---------+\n",
      "|  sumit| 30|bangalore|\n",
      "|  kapil| 32|hyderabad|\n",
      "|sathish| 16|  chennai|\n",
      "|   ravi| 39|bangalore|\n",
      "| kavita| 12|hyderabad|\n",
      "|  kavya| 19|   mysore|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age check function \n",
    "def age_check(age):\n",
    "    if age > 18:\n",
    "        return \"Y\"\n",
    "    else:\n",
    "        return \"N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the function as UDF \n",
    "parse_age_function = F.udf(age_check, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+-----+\n",
      "|   name|age|     city|Adult|\n",
      "+-------+---+---------+-----+\n",
      "|  sumit| 30|bangalore|    Y|\n",
      "|  kapil| 32|hyderabad|    Y|\n",
      "|sathish| 16|  chennai|    N|\n",
      "|   ravi| 39|bangalore|    Y|\n",
      "| kavita| 12|hyderabad|    N|\n",
      "|  kavya| 19|   mysore|    Y|\n",
      "+-------+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We need to create a new column \n",
    "df2 = df1.withColumn('Adult', parse_age_function(df1.age))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
